#!/usr/bin/env python3
"""
scripts/fetch_assets.py
DRG Bosco Terminal â€” ê²Œì„ ì—ì…‹ ìë™ ë‹¤ìš´ë¡œë”

ì†ŒìŠ¤: Deep Rock Galactic Wiki (deeprockgalactic.fandom.com)
      MediaWiki APIë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ê²€ìƒ‰ í›„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  python scripts/fetch_assets.py                          # ì „ì²´ ë‹¤ìš´ë¡œë“œ
  python scripts/fetch_assets.py --dry-run               # URLë§Œ í™•ì¸ (ë‹¤ìš´ë¡œë“œ ì•ˆ í•¨)
  python scripts/fetch_assets.py --missing               # ëˆ„ë½ëœ íŒŒì¼ë§Œ ë‹¤ìš´ë¡œë“œ
  python scripts/fetch_assets.py --category biomes       # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ
  python scripts/fetch_assets.py --category missions
  python scripts/fetch_assets.py --category mutators
  python scripts/fetch_assets.py --category warnings
"""

import io
import re
import sys
import time
import argparse
import requests
from pathlib import Path

# Windows í„°ë¯¸ë„ UTF-8 ì¶œë ¥ ê°•ì œ
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WIKI_API = "https://deeprockgalactic.fandom.com/api.php"
REQUEST_DELAY = 0.6   # API ìš”ì²­ ê°„ê²© (ì´ˆ) â€” ê³¼ë¶€í•˜ ë°©ì§€, ë„ˆë¬´ ë‚®ì¶”ì§€ ë§ˆì„¸ìš”

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "DRG-BoscoTerminal-AssetFetcher/1.0 (fan app, non-commercial)",
    "Accept": "application/json",
})

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ì¶œë ¥ ê²½ë¡œ
BASE_DIR = Path(__file__).parent.parent
OUTPUT_DIRS = {
    "biomes":   BASE_DIR / "assets" / "images" / "biomes",
    "missions": BASE_DIR / "assets" / "images" / "missions",
    "mutators": BASE_DIR / "assets" / "icons"  / "mutators",
    "warnings": BASE_DIR / "assets" / "icons"  / "warnings",
}

# â”€â”€ ì´ë¯¸ì§€ ëª©ë¡ (strings.dart ê¸°ì¤€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BIOMES: list[str] = [
    "Azure Weald",
    "Crystalline Caverns",
    "Dense Biozone",
    "Fungus Bogs",
    "Glacial Strata",
    "Hollow Bough",
    "Magma Core",
    "Radioactive Exclusion Zone",
    "Salt Pits",
    "Sandblasted Corridors",
    "Ossuary Depths",
]

MISSIONS: list[str] = [
    "Mining Expedition",
    "Egg Hunt",
    "On-Site Refining",
    "Point Extraction",
    "Salvage Operation",
    "Escort Duty",
    "Elimination",
    "Industrial Sabotage",
    "Deep Scan",
    "Heavy Excavation",
]

MUTATORS: list[str] = [
    "Double XP",
    "Gold Rush",
    "Mineral Mania",
    "Low Gravity",
    "Rich Atmosphere",
    "Critical Weakness",
    "Golden Bugs",
    "Volatile Guts",
    "Shield Disruption",
]

WARNINGS: list[str] = [
    "Mactera Plague",
    "Cave Leech Cluster",
    "Parasites",
    "Exploder Infestation",
    "Lethal Enemies",
    "Low Oxygen",
    "Haunted Cave",
    "Elite Threat",
    "Swarmageddon",
    "Rival Presence",
    "Lithophage Outbreak",
]

# â”€â”€ Wiki íŒŒì¼ëª… ì§ì ‘ ì§€ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¼ì§€ ê²€ìƒ‰ë³´ë‹¤ ì •í™•í•œ íŒŒì¼ëª…ì„ ì•Œê³  ìˆì„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ê²€ìƒ‰ ì „ì— ì´ íŒŒì¼ëª…ìœ¼ë¡œ ë¨¼ì € ì‹œë„í•©ë‹ˆë‹¤.
DIRECT_WIKI_FILES: dict[str, str] = {
    # Mission icons (API ê²€ì¦ ì™„ë£Œ)
    "Egg Hunt":                  "Egg Hunt icon u7.png",
    "On-Site Refining":          "Refining icon.png",
    "Salvage Operation":         "Salvage icon.png",
    "Escort Duty":               "Escort icon.png",
    "Industrial Sabotage":       "Sabotage icon.png",
    # ì•„ë˜ í•­ëª©ì€ wikiì— íŒŒì¼ ì—†ìŒ â†’ MANUAL_URLSì— ì§ì ‘ ì¶”ê°€í•˜ê±°ë‚˜ ë‚˜ì¤‘ì— ìˆ˜ë™ ì €ì¥
    # "Deep Scan":               ???   (ìœ„í‚¤ ë¯¸ë“±ë¡, Season 3+ ì‹ ê·œ ë¯¸ì…˜)
    # "Heavy Excavation":        ???   (ìœ„í‚¤ ë¯¸ë“±ë¡, Season 5+ ì‹ ê·œ ë¯¸ì…˜)
    # "Ossuary Depths":          ???   (ìœ„í‚¤ ë¯¸ë“±ë¡, Season 5 ë°”ì´ì˜´)
}

# â”€â”€ ìˆ˜ë™ ì§€ì • URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DIRECT_WIKI_FILESë¡œë„ ëª» ì°¾ì„ ë•Œ URLì„ ì§ì ‘ ì§€ì •í•©ë‹ˆë‹¤.
# Wiki íŒŒì¼ URL í™•ì¸: https://deeprockgalactic.fandom.com/wiki/Special:FilePath/íŒŒì¼ëª….png
MANUAL_URLS: dict[str, str] = {
    # ì˜ˆì‹œ:
    # "Ossuary Depths": "https://static.wikia.nocookie.net/.../OssuaryDepths.jpg",
}

# â”€â”€ ìœ í‹¸ë¦¬í‹° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def to_snake(name: str) -> str:
    """'Azure Weald' â†’ 'azure_weald'  /  'On-Site Refining' â†’ 'on_site_refining'"""
    s = name.strip().lower()
    s = re.sub(r"[\s\-]+", "_", s)
    s = re.sub(r"[^\w]", "", s)
    return s


def wiki_search(query: str, limit: int = 8) -> list[str]:
    """
    DRG Wiki File ë„¤ì„ìŠ¤í˜ì´ìŠ¤(ns=6)ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ëª… ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°˜í™˜ê°’: ["FileName.png", "AnotherFile.jpg", ...]
    """
    params = {
        "action": "query",
        "list": "search",
        "srsearch": query,
        "srnamespace": "6",
        "srlimit": str(limit),
        "format": "json",
    }
    try:
        r = SESSION.get(WIKI_API, params=params, timeout=15)
        r.raise_for_status()
        results = r.json().get("query", {}).get("search", [])
        return [x["title"].removeprefix("File:") for x in results]
    except Exception as e:
        print(f"    âš  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []


def wiki_file_url(filename: str) -> str | None:
    """
    íŒŒì¼ëª… â†’ wikia CDN ì‹¤ì œ ë‹¤ìš´ë¡œë“œ URL
    ì˜ˆ: "SaltPits.jpg" â†’ "https://static.wikia.nocookie.net/..."
    """
    params = {
        "action": "query",
        "titles": f"File:{filename}",
        "prop": "imageinfo",
        "iiprop": "url",
        "format": "json",
    }
    try:
        r = SESSION.get(WIKI_API, params=params, timeout=15)
        r.raise_for_status()
        pages = r.json().get("query", {}).get("pages", {})
        for page in pages.values():
            info = page.get("imageinfo", [])
            if info:
                return info[0].get("url")
    except Exception as e:
        print(f"    âš  URL ì¡°íšŒ ì˜¤ë¥˜: {e}")
    return None


def pick_best(
    candidates: list[str],
    name: str,
    prefer_ext: tuple[str, ...],
    exclude_keywords: tuple[str, ...] = ("icon", "thumbnail", "thumb", "logo"),
    require_keywords: tuple[str, ...] = (),
) -> str | None:
    """
    í›„ë³´ íŒŒì¼ ëª©ë¡ì—ì„œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ íŒŒì¼ì„ ì„ íƒí•©ë‹ˆë‹¤.
    1) require_keywords ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ê²ƒ ìš°ì„ 
    2) ì´ë¦„ì´ í¬í•¨ëœ ê²ƒ ìš°ì„ 
    3) ì„ í˜¸ í™•ì¥ì ìš°ì„ 
    """
    name_lower = name.lower()
    pool = candidates[:]

    # í•„ìˆ˜ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í•„í„°
    if require_keywords:
        filtered = [f for f in pool if any(kw in f.lower() for kw in require_keywords)]
        if filtered:
            pool = filtered

    # ì´ë¦„ì´ í¬í•¨ëœ ê²ƒ ìš°ì„ 
    name_matched = [f for f in pool if name_lower in f.lower().replace("_", " ")]
    if name_matched:
        pool = name_matched

    # ì„ í˜¸ í™•ì¥ì ìš°ì„  ì ìš©
    for ext in prefer_ext:
        for f in pool:
            if f.lower().split("?")[0].endswith(ext):
                return f

    return pool[0] if pool else None


def download_file(url: str, dest: Path) -> bool:
    """URLì—ì„œ dest ê²½ë¡œë¡œ ë°”ì´ë„ˆë¦¬ ë‹¤ìš´ë¡œë“œ"""
    try:
        r = SESSION.get(url, timeout=30, stream=True)
        r.raise_for_status()
        dest.parent.mkdir(parents=True, exist_ok=True)
        with open(dest, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        size_kb = dest.stat().st_size // 1024
        print(f"    âœ“ ì €ì¥: {dest.relative_to(BASE_DIR)}  ({size_kb} KB)")
        return True
    except Exception as e:
        print(f"    âœ— ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False


# â”€â”€ ì¹´í…Œê³ ë¦¬ë³„ ë‹¤ìš´ë¡œë“œ ì „ëµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CATEGORY_CONFIG: dict[str, dict] = {
    "biomes": {
        "names": BIOMES,
        # ë°”ì´ì˜´ ë°°ê²½ ì´ë¯¸ì§€: ë¡œë”©í™”ë©´ ìŠ¤íƒ€ì¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì›í•¨
        "search_suffixes": ["{name}", "{name} biome", "{name} cave"],
        "prefer_ext": (".jpg", ".png", ".webp"),
        "require_keywords": (),           # íŠ¹ë³„í•œ í‚¤ì›Œë“œ í•„í„° ì—†ìŒ
        "exclude_icon": False,
    },
    "missions": {
        "names": MISSIONS,
        # ë¯¸ì…˜ íƒ€ì… ì•„ì´ì½˜
        "search_suffixes": ["{name}", "{name} mission", "{name} icon"],
        "prefer_ext": (".png", ".webp", ".jpg"),
        "require_keywords": (),
        "exclude_icon": False,
    },
    "mutators": {
        "names": MUTATORS,
        # ë³€ì´ ì•„ì´ì½˜ â€” ì‘ì€ PNG ì•„ì´ì½˜ ì„ í˜¸
        "search_suffixes": ["{name}", "{name} mutator", "{name} icon"],
        "prefer_ext": (".png", ".webp"),
        "require_keywords": (),
        "exclude_icon": False,
    },
    "warnings": {
        "names": WARNINGS,
        # ê²½ê³  ì•„ì´ì½˜
        "search_suffixes": ["{name}", "{name} warning", "{name} anomaly"],
        "prefer_ext": (".png", ".webp"),
        "require_keywords": (),
        "exclude_icon": False,
    },
}


def fetch_category(
    category: str,
    dry_run: bool = False,
    missing_only: bool = False,
) -> dict[str, list[str]]:
    """
    í•œ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    ë°˜í™˜: {"ok": [...], "fail": [...], "skip": [...]}
    """
    cfg = CATEGORY_CONFIG[category]
    names: list[str] = cfg["names"]
    out_dir: Path = OUTPUT_DIRS[category]
    out_dir.mkdir(parents=True, exist_ok=True)

    stats: dict[str, list[str]] = {"ok": [], "fail": [], "skip": []}

    for name in names:
        snake = to_snake(name)
        print(f"\n  [{name}]")

        # â‘  ìˆ˜ë™ URLì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‚¬ìš© (ìµœìš°ì„ )
        if name in MANUAL_URLS:
            url = MANUAL_URLS[name]
            ext = Path(url.split("?")[0]).suffix or ".png"
            dest = out_dir / f"{snake}{ext}"
            if dry_run:
                print(f"    [dry-run] MANUAL: {url}")
                stats["ok"].append(name)
            elif download_file(url, dest):
                stats["ok"].append(name)
            else:
                stats["fail"].append(name)
            continue

        # â‘¡ missing_only ëª¨ë“œ: ì´ë¯¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if missing_only:
            existing = list(out_dir.glob(f"{snake}.*"))
            if existing:
                print(f"    â†’ ìŠ¤í‚µ (ê¸°ì¡´ íŒŒì¼: {existing[0].name})")
                stats["skip"].append(name)
                continue

        # â‘¢ DIRECT_WIKI_FILES: ì•Œë ¤ì§„ íŒŒì¼ëª… ì§ì ‘ ì¡°íšŒ (ê²€ìƒ‰ë³´ë‹¤ ì •í™•)
        chosen_file: str | None = None
        cached_url: str | None = None   # ì§ì ‘ ì¡°íšŒ ì‹œ URL ì¬ì‚¬ìš©

        if name in DIRECT_WIKI_FILES:
            direct_name = DIRECT_WIKI_FILES[name]
            cached_url = wiki_file_url(direct_name)
            time.sleep(REQUEST_DELAY)
            if cached_url:
                chosen_file = direct_name
                print(f"    â†’ ì§ì ‘ ì§€ì •: {chosen_file}")
            else:
                print(f"    âš  ì§ì ‘ ì§€ì • íŒŒì¼ ì—†ìŒ ({direct_name}), ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜...")

        # â‘£ Wiki ê²€ìƒ‰ (ì§ì ‘ ì§€ì • ì‹¤íŒ¨ ì‹œ í´ë°±)
        if not chosen_file:
            for suffix_tmpl in cfg["search_suffixes"]:
                query = suffix_tmpl.format(name=name)
                candidates = wiki_search(query)
                time.sleep(REQUEST_DELAY)

                if candidates:
                    chosen_file = pick_best(
                        candidates,
                        name,
                        prefer_ext=cfg["prefer_ext"],
                        require_keywords=cfg["require_keywords"],
                    )
                    if chosen_file:
                        print(f"    â†’ ê²€ìƒ‰ ê²°ê³¼: {chosen_file}")
                        break

        if not chosen_file:
            print(f"    âœ— ì°¾ê¸° ì‹¤íŒ¨ â€” MANUAL_URLS ë˜ëŠ” DIRECT_WIKI_FILESì— ì¶”ê°€ í•„ìš”")
            stats["fail"].append(name)
            continue

        # â‘¤ CDN URL íšë“ (ì§ì ‘ ì¡°íšŒ ì‹œ ì¬ì‚¬ìš©, ê²€ìƒ‰ ê²°ê³¼ëŠ” ì‹ ê·œ ì¡°íšŒ)
        url = cached_url or wiki_file_url(chosen_file)
        if not cached_url:
            time.sleep(REQUEST_DELAY)

        if not url:
            print(f"    âœ— URL íšë“ ì‹¤íŒ¨")
            stats["fail"].append(name)
            continue

        # í™•ì¥ì ì¶”ì¶œ (ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ì œê±°)
        ext = Path(url.split("?")[0]).suffix or ".png"
        dest = out_dir / f"{snake}{ext}"

        if dry_run:
            print(f"    [dry-run] {url}")
            print(f"           â†’ {dest.relative_to(BASE_DIR)}")
            stats["ok"].append(name)
        else:
            if download_file(url, dest):
                stats["ok"].append(name)
            else:
                stats["fail"].append(name)

    return stats


# â”€â”€ ê²°ê³¼ ë¦¬í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def print_report(all_stats: dict[str, dict[str, list[str]]]) -> None:
    total_ok = total_fail = total_skip = 0
    failed_items: list[tuple[str, str]] = []

    print(f"\n{'='*60}")
    print("  ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")

    for cat, stats in all_stats.items():
        ok    = len(stats["ok"])
        fail  = len(stats["fail"])
        skip  = len(stats["skip"])
        total_ok   += ok
        total_fail += fail
        total_skip += skip
        print(f"  {cat:12} âœ“{ok:3}  âœ—{fail:3}  ìŠ¤í‚µ{skip:3}")
        for name in stats["fail"]:
            failed_items.append((cat, name))

    print(f"{'â”€'*60}")
    print(f"  í•©ê³„          âœ“{total_ok:3}  âœ—{total_fail:3}  ìŠ¤í‚µ{total_skip:3}")
    print(f"{'='*60}")

    if failed_items:
        print("\nâš  ìˆ˜ë™ ì²˜ë¦¬ê°€ í•„ìš”í•œ í•­ëª©:")
        print("  ì•„ë˜ íŒŒì¼ì„ Wikiì—ì„œ ì§ì ‘ ì°¾ì•„ MANUAL_URLSì— ì¶”ê°€í•˜ê±°ë‚˜,")
        print("  íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•œ í›„ í•´ë‹¹ ê²½ë¡œì— ì €ì¥í•˜ì„¸ìš”.\n")
        for cat, name in failed_items:
            snake = to_snake(name)
            out_path = OUTPUT_DIRS[cat] / f"{snake}.png"
            wiki_search_url = (
                "https://deeprockgalactic.fandom.com/wiki/Special:Search"
                f"?query={requests.utils.quote(name)}&scope=internal&contentType=file"
            )
            print(f"  â€¢ [{cat}] {name}")
            print(f"      ì €ì¥ ìœ„ì¹˜: {out_path.relative_to(BASE_DIR)}")
            print(f"      Wiki ê²€ìƒ‰: {wiki_search_url}")


# â”€â”€ ì§„ì…ì  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    parser = argparse.ArgumentParser(
        description="DRG Bosco Terminal ì—ì…‹ ë‹¤ìš´ë¡œë” â€” Wikiì—ì„œ ì´ë¯¸ì§€ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤."
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ì—†ì´ URLë§Œ í™•ì¸í•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--missing", action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê³  ëˆ„ë½ëœ ê²ƒë§Œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.",
    )
    parser.add_argument(
        "--category",
        choices=list(CATEGORY_CONFIG.keys()),
        help="íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.",
    )
    args = parser.parse_args()

    print("=" * 60)
    print("  DRG Bosco Terminal â€” ì—ì…‹ ë‹¤ìš´ë¡œë”")
    print("  ì†ŒìŠ¤: deeprockgalactic.fandom.com (MediaWiki API)")
    if args.dry_run:
        print("  ëª¨ë“œ: DRY-RUN (ë‹¤ìš´ë¡œë“œ ì—†ìŒ)")
    if args.missing:
        print("  ëª¨ë“œ: MISSING ONLY (ëˆ„ë½ íŒŒì¼ë§Œ)")
    print("=" * 60)

    categories = [args.category] if args.category else list(CATEGORY_CONFIG.keys())
    all_stats: dict[str, dict[str, list[str]]] = {}

    for cat in categories:
        names_count = len(CATEGORY_CONFIG[cat]["names"])
        print(f"\n{'â”€'*60}")
        print(f"  ğŸ“ {cat.upper()} â€” {names_count}ê°œ")
        print(f"{'â”€'*60}")

        all_stats[cat] = fetch_category(
            cat,
            dry_run=args.dry_run,
            missing_only=args.missing,
        )

    print_report(all_stats)

    if args.dry_run:
        print("\nğŸ’¡ ì‹¤ì œ ë‹¤ìš´ë¡œë“œí•˜ë ¤ë©´ --dry-run ì˜µì…˜ì„ ì œê±°í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”.")


if __name__ == "__main__":
    main()
